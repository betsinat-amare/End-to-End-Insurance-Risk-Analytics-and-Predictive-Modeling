{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5758a00",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/cleaned_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load cleaned data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/cleaned_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create metrics\u001b[39;00m\n\u001b[1;32m     11\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClaimFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotalClaims\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/cleaned_data.csv'"
     ]
    }
   ],
   "source": [
    "# src/hypothesis_testing.py\n",
    "\"\"\"\n",
    "Task 3 - A/B Hypothesis Testing\n",
    "Saves results to results/hypothesis_results.csv and plots to results/*.png\n",
    "\n",
    "Tests included:\n",
    "- Chi-square: Claim Frequency vs Province\n",
    "- Chi-square: Claim Frequency vs Top N PostalCode (Zip)\n",
    "- ANOVA (or Kruskal-Wallis): Margin across top N PostalCode\n",
    "- Chi-square: Claim Frequency vs Gender\n",
    "- t-test (Welch): Margin by Gender\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency, f_oneway, kruskal, ttest_ind\n",
    "import statsmodels.stats.multicomp as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "INPUT_CANDIDATES = [\n",
    "    \"data/processed/cleaned_data_sample.csv\",\n",
    "    \"data/processed/cleaned_data.csv\"\n",
    "]\n",
    "RESULTS_DIR = \"results\"\n",
    "TOP_N_ZIPCODES = 10     # for zipcode-based tests, use top N zipcodes by count\n",
    "ALPHA = 0.05\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Load data (sample first, fallback to full)\n",
    "# -----------------------\n",
    "df = None\n",
    "for p in INPUT_CANDIDATES:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"Loading {p}\")\n",
    "        # file may be pipe-separated or comma; try both\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(p, sep=\"|\", engine=\"python\")\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"No processed data found. Run preprocess.py first.\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Ensure numeric columns\n",
    "for col in [\"TotalPremium\", \"TotalClaims\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# -----------------------\n",
    "# Create metrics\n",
    "# -----------------------\n",
    "df[\"ClaimFrequency\"] = (df[\"TotalClaims\"] > 0).astype(int)\n",
    "df[\"Margin\"] = df[\"TotalPremium\"] - df[\"TotalClaims\"]\n",
    "df[\"Severity\"] = df[\"TotalClaims\"].where(df[\"TotalClaims\"] > 0, np.nan)\n",
    "# small check\n",
    "n_rows = len(df)\n",
    "print(f\"Rows loaded: {n_rows}\")\n",
    "\n",
    "# Helper to record results\n",
    "results = []\n",
    "\n",
    "# -----------------------\n",
    "# Utility functions\n",
    "# -----------------------\n",
    "def chi2_test_category_vs_freq(data, category_col, min_count=5):\n",
    "    \"\"\"\n",
    "    Chi-square on contingency table of category_col vs ClaimFrequency.\n",
    "    Returns statistic, p-value, conclusion, contingency table shape info.\n",
    "    \"\"\"\n",
    "    table = pd.crosstab(data[category_col].fillna(\"MISSING\"), data[\"ClaimFrequency\"])\n",
    "    # remove rows with very small counts?\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    conclusion = \"Reject H0\" if p < ALPHA else \"Fail to reject H0\"\n",
    "    return dict(\n",
    "        test=\"chi2\",\n",
    "        feature=category_col,\n",
    "        stat=float(stat),\n",
    "        p_value=float(p),\n",
    "        conclusion=conclusion,\n",
    "        size_rows=table.shape[0],\n",
    "        size_cols=table.shape[1],\n",
    "    )\n",
    "\n",
    "def anova_margin_across_groups(data, group_col, top_n=None):\n",
    "    \"\"\"\n",
    "    One-way ANOVA on Margin across groups in group_col.\n",
    "    If many groups, restrict to top_n groups by count.\n",
    "    If ANOVA assumptions seem violated (non-normal / heteroscedastic), run Kruskal-Wallis as robust alternative.\n",
    "    Returns dict with results; if ANOVA significant, also runs Tukey HSD for pairwise comparisons.\n",
    "    \"\"\"\n",
    "    grp = data[[group_col, \"Margin\"]].dropna()\n",
    "    counts = grp[group_col].value_counts()\n",
    "    if top_n:\n",
    "        top_groups = counts.head(top_n).index.tolist()\n",
    "        grp = grp[grp[group_col].isin(top_groups)]\n",
    "    groups = [g[\"Margin\"].dropna().values for _, g in grp.groupby(group_col)]\n",
    "    # Need at least 2 groups with data\n",
    "    if len(groups) < 2:\n",
    "        return {\"test\":\"anova\",\"feature\":group_col,\"message\":\"Not enough groups\"}\n",
    "    # ANOVA\n",
    "    try:\n",
    "        f_stat, p = f_oneway(*groups)\n",
    "    except Exception as e:\n",
    "        # fallback to kruskal\n",
    "        kw_stat, kw_p = kruskal(*groups)\n",
    "        conclusion = \"Reject H0\" if kw_p < ALPHA else \"Fail to reject H0\"\n",
    "        return {\"test\":\"kruskal\",\"feature\":group_col,\"stat\":float(kw_stat),\"p_value\":float(kw_p),\"conclusion\":conclusion,\"n_groups\":len(groups)}\n",
    "    conclusion = \"Reject H0\" if p < ALPHA else \"Fail to reject H0\"\n",
    "    res = {\"test\":\"anova\",\"feature\":group_col,\"stat\":float(f_stat),\"p_value\":float(p),\"conclusion\":conclusion,\"n_groups\":len(groups)}\n",
    "    # If significant, run Tukey HSD (pairwise)\n",
    "    if p < ALPHA:\n",
    "        try:\n",
    "            tukey = mc.pairwise_tukeyhsd(endog=grp[\"Margin\"], groups=grp[group_col], alpha=ALPHA)\n",
    "            # convert tukey summary to DataFrame\n",
    "            tukey_df = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n",
    "            res[\"tukey_summary\"] = tukey_df\n",
    "        except Exception as e:\n",
    "            res[\"tukey_summary\"] = f\"Failed to run Tukey: {e}\"\n",
    "    return res\n",
    "\n",
    "def ttest_margin_between_two_groups(data, group_col, groupA, groupB, metric=\"Margin\"):\n",
    "    \"\"\"\n",
    "    Welch's t-test between two groups\n",
    "    \"\"\"\n",
    "    a = data[data[group_col]==groupA][metric].dropna()\n",
    "    b = data[data[group_col]==groupB][metric].dropna()\n",
    "    if len(a) < 5 or len(b) < 5:\n",
    "        return {\"test\":\"ttest\",\"feature\":f\"{group_col}:{groupA} vs {groupB}\",\"message\":\"Insufficient samples\"}\n",
    "    stat, p = ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n",
    "    conclusion = \"Reject H0\" if p < ALPHA else \"Fail to reject H0\"\n",
    "    return {\"test\":\"ttest\",\"feature\":f\"{group_col}:{groupA} vs {groupB}\",\"stat\":float(stat),\"p_value\":float(p),\"conclusion\":conclusion,\"n_a\":len(a),\"n_b\":len(b)}\n",
    "\n",
    "# -----------------------\n",
    "# TEST A: H0: There are no risk differences across provinces\n",
    "# Use chi-square on ClaimFrequency vs Province\n",
    "# -----------------------\n",
    "if \"Province\" in df.columns:\n",
    "    print(\"\\nRunning chi-square: ClaimFrequency vs Province\")\n",
    "    a_res = chi2_test_category_vs_freq(df, \"Province\")\n",
    "    results.append(a_res)\n",
    "    # plot average loss ratio by province\n",
    "    loss_by_prov = df.groupby(\"Province\").apply(lambda d: d[\"TotalClaims\"].sum()/d[\"TotalPremium\"].sum() if d[\"TotalPremium\"].sum()>0 else np.nan).sort_values(ascending=False)\n",
    "    loss_by_prov.plot(kind=\"bar\", figsize=(12,5))\n",
    "    plt.ylabel(\"Loss Ratio (TotalClaims/TotalPremium)\")\n",
    "    plt.title(\"Loss Ratio by Province\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR,\"lossratio_by_province.png\"))\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Province column not found; skipping province test\")\n",
    "\n",
    "# -----------------------\n",
    "# TEST B: H0: There are no risk differences between zip codes\n",
    "# For practicality we will analyze top N zipcodes by record count\n",
    "# Use chi-square on freq vs postalcode (top N)\n",
    "# -----------------------\n",
    "if \"PostalCode\" in df.columns:\n",
    "    print(f\"\\nRunning chi-square: ClaimFrequency vs top {TOP_N_ZIPCODES} PostalCode\")\n",
    "    top_zips = df[\"PostalCode\"].value_counts().head(TOP_N_ZIPCODES).index.tolist()\n",
    "    df_topzip = df[df[\"PostalCode\"].isin(top_zips)].copy()\n",
    "    if df_topzip.shape[0] < 50:\n",
    "        results.append({\"test\":\"chi2\",\"feature\":\"PostalCode_topN\",\"message\":\"Too few rows for top zip test\"})\n",
    "    else:\n",
    "        b_res = chi2_test_category_vs_freq(df_topzip, \"PostalCode\")\n",
    "        results.append(b_res)\n",
    "        # plot\n",
    "        loss_by_zip = df_topzip.groupby(\"PostalCode\").apply(lambda d: d[\"TotalClaims\"].sum()/d[\"TotalPremium\"].sum() if d[\"TotalPremium\"].sum()>0 else np.nan).sort_values(ascending=False)\n",
    "        loss_by_zip.plot(kind=\"bar\", figsize=(12,5))\n",
    "        plt.ylabel(\"Loss Ratio (TotalClaims/TotalPremium)\")\n",
    "        plt.title(f\"Loss Ratio by Top {TOP_N_ZIPCODES} Postal Codes\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR,\"lossratio_top_zipcodes.png\"))\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"PostalCode column not found; skipping zipcode test\")\n",
    "\n",
    "# -----------------------\n",
    "# TEST C: H0: There is no significant margin (profit) difference between zip codes\n",
    "# We'll run ANOVA on Margin across top N zipcodes\n",
    "# -----------------------\n",
    "if \"PostalCode\" in df.columns:\n",
    "    print(f\"\\nRunning ANOVA on Margin across top {TOP_N_ZIPCODES} PostalCodes\")\n",
    "    anova_res = anova_margin_across_groups(df, \"PostalCode\", top_n=TOP_N_ZIPCODES)\n",
    "    # If Tukey exists, we will save it\n",
    "    if isinstance(anova_res, dict) and \"tukey_summary\" in anova_res and isinstance(anova_res[\"tukey_summary\"], pd.DataFrame):\n",
    "        anova_res[\"tukey_summary\"].to_csv(os.path.join(RESULTS_DIR, \"tukey_margin_posthoc_top_zipcodes.csv\"), index=False)\n",
    "    results.append(anova_res)\n",
    "else:\n",
    "    print(\"PostalCode column not found; skipping margin-by-zipcode ANOVA\")\n",
    "\n",
    "# -----------------------\n",
    "# TEST D: H0: There is no significant risk difference between Women and Men\n",
    "# Use chi-square for ClaimFrequency vs Gender; t-test for Margin by Gender\n",
    "# -----------------------\n",
    "if \"Gender\" in df.columns:\n",
    "    print(\"\\nRunning chi-square: ClaimFrequency vs Gender\")\n",
    "    chi_gender = chi2_test_category_vs_freq(df, \"Gender\")\n",
    "    results.append(chi_gender)\n",
    "\n",
    "    # t-test on Margin (Welch)\n",
    "    genders = df[\"Gender\"].dropna().unique()\n",
    "    if len(genders) >= 2:\n",
    "        gvals = [g for g in genders if str(g).strip() != \"\"]\n",
    "        if len(gvals) >= 2:\n",
    "            # pick first two (typically 'M' and 'F' or 'Male','Female')\n",
    "            gA, gB = gvals[0], gvals[1]\n",
    "            print(f\"\\nRunning Welch t-test for Margin between {gA} and {gB}\")\n",
    "            t_res = ttest_margin_between_two_groups(df, \"Gender\", gA, gB, metric=\"Margin\")\n",
    "            results.append(t_res)\n",
    "        else:\n",
    "            results.append({\"test\":\"ttest\",\"feature\":\"Gender_margin\",\"message\":\"Not enough gender categories for t-test\"})\n",
    "    else:\n",
    "        results.append({\"test\":\"ttest\",\"feature\":\"Gender_margin\",\"message\":\"Not enough gender data for t-test\"})\n",
    "else:\n",
    "    print(\"Gender column not found; skipping gender tests\")\n",
    "\n",
    "# -----------------------\n",
    "# Save results summary to CSV\n",
    "# -----------------------\n",
    "def normalise_result(r):\n",
    "    # If dict contains tukey_summary as dataframe, save path and remove heavy data\n",
    "    r = r.copy()\n",
    "    if \"tukey_summary\" in r and isinstance(r[\"tukey_summary\"], pd.DataFrame):\n",
    "        r[\"tukey_summary_file\"] = os.path.join(RESULTS_DIR, \"tukey_margin_posthoc_top_zipcodes.csv\")\n",
    "        del r[\"tukey_summary\"]\n",
    "    return r\n",
    "\n",
    "results_norm = [normalise_result(r) if isinstance(r, dict) else {\"result\":str(r)} for r in results]\n",
    "res_df = pd.DataFrame(results_norm)\n",
    "res_csv = os.path.join(RESULTS_DIR, \"hypothesis_results.csv\")\n",
    "res_df.to_csv(res_csv, index=False)\n",
    "print(f\"\\nSaved hypothesis test summary to {res_csv}\")\n",
    "\n",
    "# Also print nicely\n",
    "print(\"\\n--- Test Summary ---\")\n",
    "print(res_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nCompleted Task 3 hypothesis tests. Check the results/ folder for plots and CSV outputs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
